<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<style>
div.ml1 {
  margin-left: 30px;
}
ul.a {list-style-type: circle;}
</style>

<html>
	<head>
		<title>MISP Challenge-Task2 Software</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<div id="header">

					<!-- Logo -->
					<!-- <img src="images/isca.png" width="6%" height="6%" style="margin:0 77% 0 0;"> -->
					<!-- <h1 style="font-size: 120%;  margin: -4% 0 -2% 0"><a href="index.html" id="logo">Multimodal Information
							Based Speech Processing (MISP) Challenge 2022</a></h1> -->
					<h1 style="font-size: 120%;  margin: -1% 0% -1% 0%;"><a href="index.html" id="logo">Multimodal Information
								Based Speech Processing (MISP) Challenge 2022</a></h1> 	
					<img src="images/ieee.svg" width="7%"height="7%" style="margin:-10% 0% 3% 79%;">
					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="overview.html">Overview</a></li>
								<li><a href="data.html">Data</a></li>
								<li>
									<a href="#">Track1</a>
									<ul>
										<li><a href="task1_instructions.html">Instructions</a></li>										
										<li><a href="task1_software.html">Software</a></li>
						                <li><a href="task1_leaderboard.html">Leaderboard</a></li>
									</ul>
								</li>
								<li class="current">
									<a href="#">Track2</a>
									<ul>
										<li><a href="task2_instructions.html">Instructions</a></li>										
										<li><a href="task2_software.html">Software</a></li>
						                <li><a href="task2_leaderboard.html">Leaderboard</a></li>
									</ul>
								</li>
								<!-- <li>
									<a href="#">Task3</a>
									<ul>
										<li><a href="task3_instructions.html">Instructions</a></li>
										<li><a href="task3_data.html">Data</a></li>
										<li><a href="task3_software.html">Software</a></li>
										<li><a href="task3_submission.html">Submission</a></li>
									</ul>
								</li> -->
								<li><a href="download.html">Registration</a></li>
								<li><a href="extral_data.html">Extra Data</a></li>
								<!-- <li><a href="results.html">Results</a></li> -->
								<!-- <li><a href="contact.html">Contact</a></li> -->
							</ul>
						</nav>

				</div>

			<!-- Main -->
			<section class="wrapper style1">
					<div class="container">
						<h1 style="font-size: 150%;">Track2 AVDR Baseline</h1>
						<p>For details, please refer to <a href="https://github.com/mispchallenge/misp2022_baseline/tree/main/track2_AVDR"> Github link </a></p>					   

						<h1 style="font-size: 150%;">AVSD Module Training</h1>
						<p>The training of AVSD module is consistent with the track1.</p>

						<h1 style="font-size: 150%;">AVSR Module Training</h1>
						<p>The training of AVSR module uses the oracle diarization results.</p>
						<ol type="a">
							<li><strong>Data preparation</strong></li>
							<ul>
								<li><strong>speech enhancement</strong></li>
								<p>Weighted Prediction Error(WPE) dereverberation and BeamformIt are used to reduce the reverberations of speech signals. The algorithms are implemented with open-source toolkit, <a href="https://github.com/fgnt/nara_wpe">nara_wpe</a> and <a href="https://github.com/xanguera/BeamformIt">BeamformIt</a>.</p>
							</ul>
							
							<ul>
								<li><strong>prepare data and language directory for kaldi</strong></li>
								<p>For training, development, and test sets, we prepare data directories and the lexicon in the format expected by  <a href="http://kaldi-asr.org/doc/data_prep.html">kaldi</a> respectively. Note that we choose <a href="https://github.com/aishell-foundation/DaCiDian.git">DaCiDian</a> raw resource and convert it to kaldi lexicon format.</p>
							</ul>

							<li>
								<p><strong>Language model</strong></p>
								<p>We segment MISP speech transcription for language model training by applying <a href="https://github.com/aishell-foundation/DaCiDian.git">DaCiDian</a> as dict and <a href="https://github.com/fxsjy/jieba">Jieba</a> open-source toolkit. For the language model, we choose a maximum entropy-based 3-gram model, which achieves the best perplexity, from n-gram(n=2,3,4) models trained on MISP speech transcripts with different smoothing algorithms and parameters sets. And the selected 3-gram model has 516600 unigrams, 432247 bigrams, and 915962 trigrams respectively.  Note that the temporary and final language models are stored in /data/srilm.</p>
							</li>

							<li>
								<p><strong>Acoustic model</strong></p>
								<p>The acoustic model of the AVSR system is built largely following the Kaldi recipes which mainly contain two stages: GMM-HMM state model and DNN-HMM model.</p>
								
								<ul>
									<li><strong>GMM-HMM</strong></li>
									<p>For features extraction, we extract 13-dimensional MFCC features plus 3-dimensional pitches. As a start point for triphone models, a monophone model is trained on a subset of 50k utterances. Then a small triphone model and a larger triphone model are consecutively trained using delta features on a subset of 100k utterances and the whole dataset respectively. In the third triphone model training process, an MLLT-based global transform is estimated iteratively on the top of LDA feature to extract independent speaker features. For the fourth triphone model, feature space maximum likelihood linear regression (fMLLR) with speaker adaptive training (SAT) is applied in the training.</p>
								</ul>
								
								<ul>
									<li><strong>DNN-HMM</strong></li>
									<p>Based on the tied-triphone state alignments from GMM, DNN is configured and trained to replace GMM. The input features are 40-dimensional FBank features with cepstral normalization and the 96 × 96 (W × H) lip ROI.</p>
								</ul>

							</li>
						</ol>


						<h1 style="font-size: 150%;">Inference</h1>
						<p>The RTTM file as the output of the AVSD module contains the information of the Session, SPK, T<sup>start</sup>, and T<sup>dur</sup>.</p>
						<p>where:</p>
						<ul>
					        <li>Session<sub>k</sub>: k-th session</li>
							<li>SPK<sub>i</sub>: i-th speaker</li>
							<li>T<sub>j</sub><sup>start</sup>: the start time of the j-th utterance for SPK<sub>i</sub></li>
							<li>T<sub>j</sub><sup>dur</sup>: the duration of the j-th utterance for SPK<sub>i</sub></li>
						</ul>
						<p>During the inference, the RTTM file is used for segmenting audio and video data in AVSR module.</p>
						<p>If speaker IDs of the final decoding results are global, you only need to caculate the CER. If you run your own model and get the local speaker IDs, you need to caculate the cpCER.</p>

						<h1 style="font-size: 150%;">Results</h1>
						<center>
							<figure style="padding:0px;border:0px; margin:10px">
							<img src="images/AVDR_baseline_result.png" style="width:300px;padding:0px;border:0" />
							</figure>
						</center>	
						<p>The AVSD+AVSR model is our AVDR baseline system.</p>

						<h1 style="font-size: 150%;">Quick start</h1>
						<ul><li><strong>Setting Local System Jobs</strong></li></ul>
						<p><code># Setting local system jobs (local CPU - no external clusters)</br>
						export train_cmd=run.pl</br>
						export decode_cmd=run.pl</code></p>
						<p></p>
						<ul>
						<li><strong>Setting  Paths</strong></li>
						</ul>
						<p><code>--- path.sh ---</br>
						# Defining Kaldi root directory</br>
						export KALDI_ROOT=</br>
						# Setting paths to useful tools</br>
						export PATH=</br>
						# Enable SRILM</br>
						. $KALDI_ROOT/tools/env.sh</br>
						# Variable needed for proper data sorting</br>
						export LC_ALL=C</p>

						<p>--- run_misp.sh ---</br>
						# Defining corpus directory</br>
						misp2022_corpus=</br>
						# Defining path to beamforIt executable file</br>
						bearmformit_path = </br>
						# Defining path to python interpreter</br>
						python_path = </br>
						# the directory to host coordinate information used to crop ROI </br>
						data_roi =</br>
						# dictionary directory </br>
						dict_dir= </code></p>  
						<p></p>
						<ul>
						<li><strong>Running Training</strong></li>
						</ul>
						<p><code>./run.sh </br>
						# options:</br>
						&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        --stage  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    -1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; change the number to start from different training stages</br></code></p>



					<h1 style="font-size: 150%;">Requirments</h1>
					<ul>
					<li>
					<p><strong>Kaldi</strong></p>
					</li>
					<li>
					<p><strong>Python Packages:</strong></br>
					numpy</br>
					tqdm</br>
					<a href="https://github.com/fxsjy/jieba">jieba</a></p>
					</li>
					<li>
					<p><strong>Other Tools:</strong></br>
					<a href="https://github.com/fgnt/nara_wpe">nara_wpe</a></br>
					<a href="https://github.com/xanguera/BeamformIt">Beamformit</a></br>
					SRILM</p>
					</li>
					</ul>
				
					<h1 style="font-size: 150%;">Citation</h1>
					<p>If you find this code useful in your research, please consider to cite the following papers:</br>
						<font size="4">
							@inproceedings{chen2022first,</br>
							title={The first multimodal information based speech processing (misp) challenge: Data, tasks, baselines and results},</br>
							author={Chen, Hang and Zhou, Hengshun and Du, Jun and Lee, Chin-Hui and Chen, Jingdong and Watanabe, Shinji and Siniscalchi, Sabato Marco and Scharenborg, Odette and Liu, Di-Yuan and Yin, Bao-Cai and others},</br>
							booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},</br>
							pages={9266--9270},</br>
							year={2022},</br>
							organization={IEEE}</br>
							}</br>
							@inproceedings{2022misptask2,</br>
							author={Chen, Hang and Du, Jun and Dai, Yusheng and Lee, Chin-Hui and Siniscalchi, Sabato Marco and Watanabe, Shinji and Scharenborg, Odette and Chen, Jingdong and Yin, Bao-Cai and Pan, jia},</br>
							booktitle={Proc. INTERSPEECH 2022},</br>
							title={Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis},</br>
							year={2022}}
						</font>
					</p>
			</div>
		</section>


			<!-- Footer -->
				<div id="footer">
					<!-- Copyright -->
						<div class="copyright">
							<ul class="menu">
								<li>&copy; All rights reserved</li><li>E-mail: mispchallenge@gmail.com</li>
							</ul>
						</div>

				</div>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
